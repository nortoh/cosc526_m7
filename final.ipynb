{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66d6f63c-8061-42bf-8e12-11f626290c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocabulary_size = 500\n",
    "global_truncate = True\n",
    "\n",
    "#Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('app_name') \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', True) \\\n",
    "    .config('spark.sql.session.timeZone', 'UTC') \\\n",
    "    .config('spark.driver.memory','16g') \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config('spark.ui.showConsoleProgress', True) \\\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ce66de4-d48f-4e79-a541-ded0378ddb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "data = spark.read.csv(\n",
    "    'train.csv', \n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    escape='\"'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6f10e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/christian/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/christian/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[Stage 526:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+--------------------+-----------+---------------+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|author|     id|                text|        cleaned_text|text_length|author_category|     lexical_density|num_words|        cleaned_text|      lemmatized_txt|        cleaned_text|\n",
      "+------+-------+--------------------+--------------------+-----------+---------------+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|   EAP|id26305|This process, how...|this process howe...|        231|          wordy|0.003099124122294439|       41|this process howe...|This process, how...|this process howe...|\n",
      "|   HPL|id17569|It never once occ...|it never once occ...|         71|          wordy|0.003865561403524783|       14|it never once occ...|It never once occ...|it never once occ...|\n",
      "|   EAP|id11008|In his left hand ...|in his left hand ...|        200|          wordy|0.003099124122294439|       36|in his left hand ...|In his left hand ...|in his left hand ...|\n",
      "|   MWS|id27763|How lovely is spr...|how lovely is spr...|        206|          wordy| 0.00242801583636171|       34|how lovely is spr...|How lovely is spr...|how lovely is spr...|\n",
      "|   HPL|id12958|Finding nothing e...|finding nothing e...|        174|          wordy|0.003865561403524783|       27|finding nothing e...|Finding nothing e...|finding nothing e...|\n",
      "|   MWS|id22965|A youth passed in...|a youth passed in...|        468|          wordy| 0.00242801583636171|       83|a youth passed in...|A youth passed in...|a youth passed in...|\n",
      "|   EAP|id09674|The astronomer, p...|the astronomer pe...|        128|          wordy|0.003099124122294439|       21|the astronomer pe...|The astronomer, p...|the astronomer pe...|\n",
      "|   EAP|id13515|The surcingle hun...|the surcingle hun...|         43|          wordy|0.003099124122294439|        8|the surcingle hun...|The surcingle hun...|the surcingle hun...|\n",
      "|   EAP|id19322|I knew that you c...|i knew that you c...|        488|          wordy|0.003099124122294439|       88|i knew that you c...|I knew that you c...|i knew that you c...|\n",
      "|   MWS|id00912|I confess that ne...|i confess that ne...|        144|          wordy| 0.00242801583636171|       23|i confess that ne...|I confess that ne...|i confess that ne...|\n",
      "|   MWS|id16737|He shall find tha...|he shall find tha...|        106|          wordy| 0.00242801583636171|       22|he shall find tha...|He shall find tha...|he shall find tha...|\n",
      "|   EAP|id16607|Here we barricade...|here we barricade...|         63|          wordy|0.003099124122294439|       10|here we barricade...|Here we barricade...|here we barricade...|\n",
      "|   HPL|id19764|Herbert West need...|herbert west need...|         87|          wordy|0.003865561403524783|       15|herbert west need...|Herbert West need...|herbert west need...|\n",
      "|   HPL|id18886|The farm like gro...|the farm like gro...|         86|          wordy|0.003865561403524783|       15|the farm like gro...|The farm like gro...|the farm like gro...|\n",
      "|   EAP|id17189|But a glance will...|but a glance will...|         48|          wordy|0.003099124122294439|       10|but a glance will...|But a glance will...|but a glance will...|\n",
      "|   MWS|id12799|He had escaped me...|he had escaped me...|        261|          wordy| 0.00242801583636171|       47|he had escaped me...|He had escaped me...|he had escaped me...|\n",
      "|   EAP|id08441|To these speeches...|to these speeches...|        340|          wordy|0.003099124122294439|       61|to these speeches...|To these speech t...|to these speeches...|\n",
      "|   MWS|id13117|Her native sprigh...|her native sprigh...|        173|          wordy| 0.00242801583636171|       28|her native sprigh...|Her native sprigh...|her native sprigh...|\n",
      "|   EAP|id14862|I even went so fa...|i even went so fa...|        270|          wordy|0.003099124122294439|       48|i even went so fa...|I even went so fa...|i even went so fa...|\n",
      "|   HPL|id20836|His facial aspect...|his facial aspect...|        305|          wordy|0.003865561403524783|       48|his facial aspect...|His facial aspect...|his facial aspect...|\n",
      "+------+-------+--------------------+--------------------+-----------+---------------+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Clean Text\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "# # Define a UDF (User-Defined Function) to remove symbols and convert to lowercase\n",
    "# def remove_symbols(text):\n",
    "#     cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "#     lowercase_text = cleaned_text.lower()\n",
    "#     return lowercase_text\n",
    "\n",
    "# # Register the UDF\n",
    "# udf_remove_symbols = udf(remove_symbols, StringType())\n",
    "# spark.udf.register(\"remove_symbols\", udf_remove_symbols)\n",
    "\n",
    "# # Define the SQLTransformer stage to apply the UDF\n",
    "# sql_transformer = SQLTransformer(\n",
    "#     statement=\"SELECT *, remove_symbols(text) AS cleaned_text FROM __THIS__\"\n",
    "# )\n",
    "\n",
    "# data = sql_transformer.transform(data)\n",
    "\n",
    "# Lemmatization of words in text\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "\n",
    "# using wordnet to lemmatize text\n",
    "# download wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# custom function to lemmatize text.  \n",
    "# text is in a series so must adjust function to accept series\n",
    "# cannot just pass text assuming it is a string\n",
    "def lemmatize(text_series):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    return text_series.apply(lambda text: ' '.join([lemm.lemmatize(x) for x in text.split()]))\n",
    "\n",
    "\n",
    "# custom pandas function to apply to pyspark df\n",
    "lemm_udf = pandas_udf(lemmatize, returnType=StringType())\n",
    "\n",
    "# add column with lemmatize data\n",
    "data = data.withColumn('lemmatized_txt', lemm_udf('text'))\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84b7d774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------------------+-----------+\n",
      "|     id|                text|author|        cleaned_text|text_length|\n",
      "+-------+--------------------+------+--------------------+-----------+\n",
      "|id26305|This process, how...|   EAP|this process howe...|        231|\n",
      "|id17569|It never once occ...|   HPL|it never once occ...|         71|\n",
      "|id11008|In his left hand ...|   EAP|in his left hand ...|        200|\n",
      "|id27763|How lovely is spr...|   MWS|how lovely is spr...|        206|\n",
      "|id12958|Finding nothing e...|   HPL|finding nothing e...|        174|\n",
      "|id22965|A youth passed in...|   MWS|a youth passed in...|        468|\n",
      "|id09674|The astronomer, p...|   EAP|the astronomer pe...|        128|\n",
      "|id13515|The surcingle hun...|   EAP|the surcingle hun...|         43|\n",
      "|id19322|I knew that you c...|   EAP|i knew that you c...|        488|\n",
      "|id00912|I confess that ne...|   MWS|i confess that ne...|        144|\n",
      "|id16737|He shall find tha...|   MWS|he shall find tha...|        106|\n",
      "|id16607|Here we barricade...|   EAP|here we barricade...|         63|\n",
      "|id19764|Herbert West need...|   HPL|herbert west need...|         87|\n",
      "|id18886|The farm like gro...|   HPL|the farm like gro...|         86|\n",
      "|id17189|But a glance will...|   EAP|but a glance will...|         48|\n",
      "|id12799|He had escaped me...|   MWS|he had escaped me...|        261|\n",
      "|id08441|To these speeches...|   EAP|to these speeches...|        340|\n",
      "|id13117|Her native sprigh...|   MWS|her native sprigh...|        173|\n",
      "|id14862|I even went so fa...|   EAP|i even went so fa...|        270|\n",
      "|id20836|His facial aspect...|   HPL|his facial aspect...|        305|\n",
      "+-------+--------------------+------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# data = data.withColumn(\"text\", func.regexp_replace(\"text\", '\"', ''))\n",
    "# data = data.withColumn('text_length', length(data['text']))\n",
    "\n",
    "# data.createOrReplaceTempView(\"spooky_sentences\")\n",
    "\n",
    "# result_df = spark.sql(\n",
    "#     '''\n",
    "#     SELECT \n",
    "#         text AS sentence,\n",
    "#         text_length,\n",
    "#         author,\n",
    "#         size(split(text, ' ')) AS word_count\n",
    "#     FROM\n",
    "#         spooky_sentences\n",
    "#     ORDER BY \n",
    "#         word_count DESC\n",
    "    \n",
    "#     '''\n",
    "# )\n",
    "\n",
    "# print(data.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30086aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+--------------------+-----------+---------------+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|author|     id|                text|        cleaned_text|text_length|author_category|     lexical_density|num_words|        cleaned_text|      lemmatized_txt|        cleaned_text|\n",
      "+------+-------+--------------------+--------------------+-----------+---------------+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|   EAP|id26305|This process, how...|this process howe...|        231|          wordy|0.003099124122294439|       41|this process howe...|This process, how...|this process howe...|\n",
      "|   HPL|id17569|It never once occ...|it never once occ...|         71|          wordy|0.003865561403524783|       14|it never once occ...|It never once occ...|it never once occ...|\n",
      "|   EAP|id11008|In his left hand ...|in his left hand ...|        200|          wordy|0.003099124122294439|       36|in his left hand ...|In his left hand ...|in his left hand ...|\n",
      "|   MWS|id27763|How lovely is spr...|how lovely is spr...|        206|          wordy| 0.00242801583636171|       34|how lovely is spr...|How lovely is spr...|how lovely is spr...|\n",
      "|   HPL|id12958|Finding nothing e...|finding nothing e...|        174|          wordy|0.003865561403524783|       27|finding nothing e...|Finding nothing e...|finding nothing e...|\n",
      "|   MWS|id22965|A youth passed in...|a youth passed in...|        468|          wordy| 0.00242801583636171|       83|a youth passed in...|A youth passed in...|a youth passed in...|\n",
      "|   EAP|id09674|The astronomer, p...|the astronomer pe...|        128|          wordy|0.003099124122294439|       21|the astronomer pe...|The astronomer, p...|the astronomer pe...|\n",
      "|   EAP|id13515|The surcingle hun...|the surcingle hun...|         43|          wordy|0.003099124122294439|        8|the surcingle hun...|The surcingle hun...|the surcingle hun...|\n",
      "|   EAP|id19322|I knew that you c...|i knew that you c...|        488|          wordy|0.003099124122294439|       88|i knew that you c...|I knew that you c...|i knew that you c...|\n",
      "|   MWS|id00912|I confess that ne...|i confess that ne...|        144|          wordy| 0.00242801583636171|       23|i confess that ne...|I confess that ne...|i confess that ne...|\n",
      "|   MWS|id16737|He shall find tha...|he shall find tha...|        106|          wordy| 0.00242801583636171|       22|he shall find tha...|He shall find tha...|he shall find tha...|\n",
      "|   EAP|id16607|Here we barricade...|here we barricade...|         63|          wordy|0.003099124122294439|       10|here we barricade...|Here we barricade...|here we barricade...|\n",
      "|   HPL|id19764|Herbert West need...|herbert west need...|         87|          wordy|0.003865561403524783|       15|herbert west need...|Herbert West need...|herbert west need...|\n",
      "|   HPL|id18886|The farm like gro...|the farm like gro...|         86|          wordy|0.003865561403524783|       15|the farm like gro...|The farm like gro...|the farm like gro...|\n",
      "|   EAP|id17189|But a glance will...|but a glance will...|         48|          wordy|0.003099124122294439|       10|but a glance will...|But a glance will...|but a glance will...|\n",
      "|   MWS|id12799|He had escaped me...|he had escaped me...|        261|          wordy| 0.00242801583636171|       47|he had escaped me...|He had escaped me...|he had escaped me...|\n",
      "|   EAP|id08441|To these speeches...|to these speeches...|        340|          wordy|0.003099124122294439|       61|to these speeches...|To these speech t...|to these speeches...|\n",
      "|   MWS|id13117|Her native sprigh...|her native sprigh...|        173|          wordy| 0.00242801583636171|       28|her native sprigh...|Her native sprigh...|her native sprigh...|\n",
      "|   EAP|id14862|I even went so fa...|i even went so fa...|        270|          wordy|0.003099124122294439|       48|i even went so fa...|I even went so fa...|i even went so fa...|\n",
      "|   HPL|id20836|His facial aspect...|his facial aspect...|        305|          wordy|0.003865561403524783|       48|his facial aspect...|His facial aspect...|his facial aspect...|\n",
      "+------+-------+--------------------+--------------------+-----------+---------------+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 537:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|author_category|count|\n",
      "+---------------+-----+\n",
      "|          wordy|19031|\n",
      "|      not wordy|  548|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# custom function to categorize authors according to text.  \n",
    "# text is in a series so must adjust function to accept series\n",
    "# cannot just pass text assuming it is a string\n",
    "def author_category(wrd_cnt):\n",
    "    \n",
    "    return wrd_cnt.apply(\n",
    "        lambda x: \"wordy\" if x > 30 else (\n",
    "            \"pity\" if x < 7 else \"not wordy\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# custom pandas function to apply to pyspark df\n",
    "auth_cat_udf = pandas_udf(author_category, returnType=StringType())\n",
    "\n",
    "# add column with author_category data\n",
    "data = data.withColumn('author_category', auth_cat_udf('text_length'))\n",
    "\n",
    "print(data)\n",
    "\n",
    "word_count=data.groupBy('author_category').count().orderBy('count',ascending=False)\n",
    "word_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4dd72b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+--------------------+-----------+---------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|author|     id|                text|        cleaned_text|text_length|author_category|     lexical_density|num_words|        cleaned_text|      lemmatized_txt|        cleaned_text|     lexical_density|\n",
      "+------+-------+--------------------+--------------------+-----------+---------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   EAP|id26305|This process, how...|this process howe...|        231|          wordy|0.003099124122294439|       41|this process howe...|This process, how...|this process howe...|0.003099124122294439|\n",
      "|   HPL|id17569|It never once occ...|it never once occ...|         71|          wordy|0.003865561403524783|       14|it never once occ...|It never once occ...|it never once occ...|0.003865561403524783|\n",
      "|   EAP|id11008|In his left hand ...|in his left hand ...|        200|          wordy|0.003099124122294439|       36|in his left hand ...|In his left hand ...|in his left hand ...|0.003099124122294439|\n",
      "|   MWS|id27763|How lovely is spr...|how lovely is spr...|        206|          wordy| 0.00242801583636171|       34|how lovely is spr...|How lovely is spr...|how lovely is spr...| 0.00242801583636171|\n",
      "|   HPL|id12958|Finding nothing e...|finding nothing e...|        174|          wordy|0.003865561403524783|       27|finding nothing e...|Finding nothing e...|finding nothing e...|0.003865561403524783|\n",
      "|   MWS|id22965|A youth passed in...|a youth passed in...|        468|          wordy| 0.00242801583636171|       83|a youth passed in...|A youth passed in...|a youth passed in...| 0.00242801583636171|\n",
      "|   EAP|id09674|The astronomer, p...|the astronomer pe...|        128|          wordy|0.003099124122294439|       21|the astronomer pe...|The astronomer, p...|the astronomer pe...|0.003099124122294439|\n",
      "|   EAP|id13515|The surcingle hun...|the surcingle hun...|         43|          wordy|0.003099124122294439|        8|the surcingle hun...|The surcingle hun...|the surcingle hun...|0.003099124122294439|\n",
      "|   EAP|id19322|I knew that you c...|i knew that you c...|        488|          wordy|0.003099124122294439|       88|i knew that you c...|I knew that you c...|i knew that you c...|0.003099124122294439|\n",
      "|   MWS|id00912|I confess that ne...|i confess that ne...|        144|          wordy| 0.00242801583636171|       23|i confess that ne...|I confess that ne...|i confess that ne...| 0.00242801583636171|\n",
      "|   MWS|id16737|He shall find tha...|he shall find tha...|        106|          wordy| 0.00242801583636171|       22|he shall find tha...|He shall find tha...|he shall find tha...| 0.00242801583636171|\n",
      "|   EAP|id16607|Here we barricade...|here we barricade...|         63|          wordy|0.003099124122294439|       10|here we barricade...|Here we barricade...|here we barricade...|0.003099124122294439|\n",
      "|   HPL|id19764|Herbert West need...|herbert west need...|         87|          wordy|0.003865561403524783|       15|herbert west need...|Herbert West need...|herbert west need...|0.003865561403524783|\n",
      "|   HPL|id18886|The farm like gro...|the farm like gro...|         86|          wordy|0.003865561403524783|       15|the farm like gro...|The farm like gro...|the farm like gro...|0.003865561403524783|\n",
      "|   EAP|id17189|But a glance will...|but a glance will...|         48|          wordy|0.003099124122294439|       10|but a glance will...|But a glance will...|but a glance will...|0.003099124122294439|\n",
      "|   MWS|id12799|He had escaped me...|he had escaped me...|        261|          wordy| 0.00242801583636171|       47|he had escaped me...|He had escaped me...|he had escaped me...| 0.00242801583636171|\n",
      "|   EAP|id08441|To these speeches...|to these speeches...|        340|          wordy|0.003099124122294439|       61|to these speeches...|To these speech t...|to these speeches...|0.003099124122294439|\n",
      "|   MWS|id13117|Her native sprigh...|her native sprigh...|        173|          wordy| 0.00242801583636171|       28|her native sprigh...|Her native sprigh...|her native sprigh...| 0.00242801583636171|\n",
      "|   EAP|id14862|I even went so fa...|i even went so fa...|        270|          wordy|0.003099124122294439|       48|i even went so fa...|I even went so fa...|i even went so fa...|0.003099124122294439|\n",
      "|   HPL|id20836|His facial aspect...|his facial aspect...|        305|          wordy|0.003865561403524783|       48|his facial aspect...|His facial aspect...|his facial aspect...|0.003865561403524783|\n",
      "+------+-------+--------------------+--------------------+-----------+---------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+--------------------+-----------+---------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|author|     id|                text|        cleaned_text|text_length|author_category|     lexical_density|num_words|        cleaned_text|      lemmatized_txt|        cleaned_text|     lexical_density|\n",
      "+------+-------+--------------------+--------------------+-----------+---------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   EAP|id26305|This process, how...|this process howe...|        231|          wordy|0.003099124122294439|       41|this process howe...|This process, how...|this process howe...|0.003099124122294439|\n",
      "|   HPL|id17569|It never once occ...|it never once occ...|         71|          wordy|0.003865561403524783|       14|it never once occ...|It never once occ...|it never once occ...|0.003865561403524783|\n",
      "|   EAP|id11008|In his left hand ...|in his left hand ...|        200|          wordy|0.003099124122294439|       36|in his left hand ...|In his left hand ...|in his left hand ...|0.003099124122294439|\n",
      "|   MWS|id27763|How lovely is spr...|how lovely is spr...|        206|          wordy| 0.00242801583636171|       34|how lovely is spr...|How lovely is spr...|how lovely is spr...| 0.00242801583636171|\n",
      "|   HPL|id12958|Finding nothing e...|finding nothing e...|        174|          wordy|0.003865561403524783|       27|finding nothing e...|Finding nothing e...|finding nothing e...|0.003865561403524783|\n",
      "|   MWS|id22965|A youth passed in...|a youth passed in...|        468|          wordy| 0.00242801583636171|       83|a youth passed in...|A youth passed in...|a youth passed in...| 0.00242801583636171|\n",
      "|   EAP|id09674|The astronomer, p...|the astronomer pe...|        128|          wordy|0.003099124122294439|       21|the astronomer pe...|The astronomer, p...|the astronomer pe...|0.003099124122294439|\n",
      "|   EAP|id13515|The surcingle hun...|the surcingle hun...|         43|          wordy|0.003099124122294439|        8|the surcingle hun...|The surcingle hun...|the surcingle hun...|0.003099124122294439|\n",
      "|   EAP|id19322|I knew that you c...|i knew that you c...|        488|          wordy|0.003099124122294439|       88|i knew that you c...|I knew that you c...|i knew that you c...|0.003099124122294439|\n",
      "|   MWS|id00912|I confess that ne...|i confess that ne...|        144|          wordy| 0.00242801583636171|       23|i confess that ne...|I confess that ne...|i confess that ne...| 0.00242801583636171|\n",
      "|   MWS|id16737|He shall find tha...|he shall find tha...|        106|          wordy| 0.00242801583636171|       22|he shall find tha...|He shall find tha...|he shall find tha...| 0.00242801583636171|\n",
      "|   EAP|id16607|Here we barricade...|here we barricade...|         63|          wordy|0.003099124122294439|       10|here we barricade...|Here we barricade...|here we barricade...|0.003099124122294439|\n",
      "|   HPL|id19764|Herbert West need...|herbert west need...|         87|          wordy|0.003865561403524783|       15|herbert west need...|Herbert West need...|herbert west need...|0.003865561403524783|\n",
      "|   HPL|id18886|The farm like gro...|the farm like gro...|         86|          wordy|0.003865561403524783|       15|the farm like gro...|The farm like gro...|the farm like gro...|0.003865561403524783|\n",
      "|   EAP|id17189|But a glance will...|but a glance will...|         48|          wordy|0.003099124122294439|       10|but a glance will...|But a glance will...|but a glance will...|0.003099124122294439|\n",
      "|   MWS|id12799|He had escaped me...|he had escaped me...|        261|          wordy| 0.00242801583636171|       47|he had escaped me...|He had escaped me...|he had escaped me...| 0.00242801583636171|\n",
      "|   EAP|id08441|To these speeches...|to these speeches...|        340|          wordy|0.003099124122294439|       61|to these speeches...|To these speech t...|to these speeches...|0.003099124122294439|\n",
      "|   MWS|id13117|Her native sprigh...|her native sprigh...|        173|          wordy| 0.00242801583636171|       28|her native sprigh...|Her native sprigh...|her native sprigh...| 0.00242801583636171|\n",
      "|   EAP|id14862|I even went so fa...|i even went so fa...|        270|          wordy|0.003099124122294439|       48|i even went so fa...|I even went so fa...|i even went so fa...|0.003099124122294439|\n",
      "|   HPL|id20836|His facial aspect...|his facial aspect...|        305|          wordy|0.003865561403524783|       48|his facial aspect...|His facial aspect...|his facial aspect...|0.003865561403524783|\n",
      "+------+-------+--------------------+--------------------+-----------+---------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 660:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---------------+---------+--------------------+\n",
      "|author|text_length|author_category|num_words|      lemmatized_txt|\n",
      "+------+-----------+---------------+---------+--------------------+\n",
      "|   EAP|        231|          wordy|       41|This process, how...|\n",
      "|   HPL|         71|          wordy|       14|It never once occ...|\n",
      "|   EAP|        200|          wordy|       36|In his left hand ...|\n",
      "|   MWS|        206|          wordy|       34|How lovely is spr...|\n",
      "|   HPL|        174|          wordy|       27|Finding nothing e...|\n",
      "|   MWS|        468|          wordy|       83|A youth passed in...|\n",
      "|   EAP|        128|          wordy|       21|The astronomer, p...|\n",
      "|   EAP|         43|          wordy|        8|The surcingle hun...|\n",
      "|   EAP|        488|          wordy|       88|I knew that you c...|\n",
      "|   MWS|        144|          wordy|       23|I confess that ne...|\n",
      "|   MWS|        106|          wordy|       22|He shall find tha...|\n",
      "|   EAP|         63|          wordy|       10|Here we barricade...|\n",
      "|   HPL|         87|          wordy|       15|Herbert West need...|\n",
      "|   HPL|         86|          wordy|       15|The farm like gro...|\n",
      "|   EAP|         48|          wordy|       10|But a glance will...|\n",
      "|   MWS|        261|          wordy|       47|He had escaped me...|\n",
      "|   EAP|        340|          wordy|       61|To these speech t...|\n",
      "|   MWS|        173|          wordy|       28|Her native sprigh...|\n",
      "|   EAP|        270|          wordy|       48|I even went so fa...|\n",
      "|   HPL|        305|          wordy|       48|His facial aspect...|\n",
      "+------+-----------+---------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size, split\n",
    "\n",
    "data.createOrReplaceTempView(\"dataset_table\")\n",
    "\n",
    "#Perform SQL query to compare lexical density across authors\n",
    "query4 = \"\"\"\n",
    "SELECT author, COUNT(DISTINCT word) / CAST(SUM(size(split(text, ' '))) AS FLOAT) AS lexical_density\n",
    "FROM (\n",
    "    SELECT author, explode(split(lower(text), ' ')) AS word, text\n",
    "    FROM dataset_table\n",
    ") subquery\n",
    "GROUP BY author\n",
    "ORDER BY lexical_density DESC\n",
    "\"\"\"\n",
    "lexical_density_comparison = spark.sql(query4)\n",
    "# lexical_density_comparison.show()\n",
    "data.drop('lexical_density')\n",
    "data = data.join(lexical_density_comparison, on='author', how='inner')\n",
    "# number of words in text\n",
    "data = data.withColumn('num_words', size(split(data['text'], ' ')))\n",
    "# lexical density times the number of words in the text\n",
    "\n",
    "# @todo fix lexical density\n",
    "print(data.select('author', 'text_length', 'author_category', 'num_words', 'lemmatized_txt'))\n",
    "# data = data.withColumn('density_x_words', data['lexical_density']*data['num_words'])\n",
    "\n",
    "# lex_dens = data.groupBy('author').agg({'lexical_density':'average'}).orderBy('avg(lexical_density)',ascending=False)\n",
    "# lex_dens.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a324ea41-bd46-4876-8bd8-f8ea0d969bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 915:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+--------------------+-----------+---------------+-------------------+---------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+-------------+\n",
      "|author|     id|                text|        cleaned_text|text_length|author_category|    lexical_density|num_words|        cleaned_text|      lemmatized_txt|        cleaned_text|    lexical_density|    lexical_density|              tokens|     filtered_tokens|            word2vec|   vectorized_tokens|               tfidf|label|   enc_author|\n",
      "+------+-------+--------------------+--------------------+-----------+---------------+-------------------+---------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+-------------+\n",
      "|   MWS|id27763|How lovely is spr...|how lovely is spr...|        206|          wordy|0.00242801583636171|       34|how lovely is spr...|How lovely is spr...|how lovely is spr...|0.00242801583636171|0.00242801583636171|[how, lovely, is,...|[lovely, spring, ...|[0.04160118444512...|          (10,[],[])|          (10,[],[])|  1.0|(2,[1],[1.0])|\n",
      "|   MWS|id22965|A youth passed in...|a youth passed in...|        468|          wordy|0.00242801583636171|       83|a youth passed in...|A youth passed in...|a youth passed in...|0.00242801583636171|0.00242801583636171|[a, youth, passed...|[youth, passed, s...|[0.03956627875886...|          (10,[],[])|          (10,[],[])|  1.0|(2,[1],[1.0])|\n",
      "|   MWS|id00912|I confess that ne...|i confess that ne...|        144|          wordy|0.00242801583636171|       23|i confess that ne...|I confess that ne...|i confess that ne...|0.00242801583636171|0.00242801583636171|[i, confess, that...|[confess, neither...|[0.02857701263080...|          (10,[],[])|          (10,[],[])|  1.0|(2,[1],[1.0])|\n",
      "|   MWS|id16737|He shall find tha...|he shall find tha...|        106|          wordy|0.00242801583636171|       22|he shall find tha...|He shall find tha...|he shall find tha...|0.00242801583636171|0.00242801583636171|[he, shall, find,...|[shall, find, fee...|[0.04062317068455...|          (10,[],[])|          (10,[],[])|  1.0|(2,[1],[1.0])|\n",
      "|   MWS|id12799|He had escaped me...|he had escaped me...|        261|          wordy|0.00242801583636171|       47|he had escaped me...|He had escaped me...|he had escaped me...|0.00242801583636171|0.00242801583636171|[he, had, escaped...|[escaped, me,, mu...|[0.03074570410778...|      (10,[9],[1.0])|(10,[9],[3.583314...|  1.0|(2,[1],[1.0])|\n",
      "|   MWS|id13117|Her native sprigh...|her native sprigh...|        173|          wordy|0.00242801583636171|       28|her native sprigh...|Her native sprigh...|her native sprigh...|0.00242801583636171|0.00242801583636171|[her, native, spr...|[native, sprightl...|[0.03677236316725...|          (10,[],[])|          (10,[],[])|  1.0|(2,[1],[1.0])|\n",
      "|   MWS|id00764|I was rich and yo...|i was rich and yo...|        201|          wordy|0.00242801583636171|       43|i was rich and yo...|I wa rich and you...|i was rich and yo...|0.00242801583636171|0.00242801583636171|[i, wa, rich, and...|[wa, rich, young,...|[0.04365574480856...|(10,[0,1,9],[2.0,...|(10,[0,1,9],[2.59...|  1.0|(2,[1],[1.0])|\n",
      "|   MWS|id00683|We could make out...|we could make out...|        278|          wordy|0.00242801583636171|       48|we could make out...|We could make out...|we could make out...|0.00242801583636171|0.00242801583636171|[we, could, make,...|[make, little, di...|[0.03154139022808...|          (10,[],[])|          (10,[],[])|  1.0|(2,[1],[1.0])|\n",
      "|   MWS|id05258|His soul overflow...|his soul overflow...|        170|          wordy|0.00242801583636171|       29|his soul overflow...|His soul overflow...|his soul overflow...|0.00242801583636171|0.00242801583636171|[his, soul, overf...|[soul, overflowed...|[0.04475335783014...|      (10,[0],[1.0])|(10,[0],[1.299282...|  1.0|(2,[1],[1.0])|\n",
      "|   MWS|id20751|The visits of Mer...|the visits of mer...|         72|          wordy|0.00242801583636171|       11|the visits of mer...|The visit of Merr...|the visits of mer...|0.00242801583636171|0.00242801583636171|[the, visit, of, ...|[visit, merrival,...|[0.04005984251853...|          (10,[],[])|          (10,[],[])|  1.0|(2,[1],[1.0])|\n",
      "+------+-------+--------------------+--------------------+-----------+---------------+-------------------+---------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Pipelining\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer, CountVectorizer, IDF, StringIndexer, OneHotEncoder, Word2Vec\n",
    "\n",
    "# # Define the SQLTransformer stage to apply the UDF\n",
    "# sql_transformer = SQLTransformer(\n",
    "#     statement=\"SELECT *, remove_symbols(sentence) AS cleaned_text FROM __THIS__\"\n",
    "# )\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"lemmatized_txt\", outputCol=\"tokens\")\n",
    "\n",
    "\n",
    "# Step 2: Stop word removal\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=20, seed=42, inputCol=\"filtered_tokens\", outputCol=\"word2vec\")\n",
    "\n",
    "# Step 3: TF-IDF calculation\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"vectorized_tokens\")\n",
    "vectorizer.setVocabSize(1) # @todo remove\n",
    "\n",
    "idf = IDF(inputCol=\"vectorized_tokens\", outputCol=\"tfidf\")\n",
    "\n",
    "indexer = StringIndexer(inputCol='author', outputCol='label')\n",
    "encoder = OneHotEncoder(inputCol='label', outputCol='enc_author')\n",
    "\n",
    "# Step 5a: Encoding author category\n",
    "# indexer1 = StringIndexer(inputCol='author_category', outputCol='idx_author_category')\n",
    "# encoder1 = OneHotEncoder(inputCol='idx_author_category', outputCol='enc_author_category')\n",
    "\n",
    "processed_pipeline = Pipeline(stages=[\n",
    "    tokenizer, \n",
    "    stopwords_remover, \n",
    "    word2Vec, \n",
    "    vectorizer, \n",
    "    idf, \n",
    "    indexer, \n",
    "    encoder\n",
    "])\n",
    "\n",
    "processed_train_data = processed_pipeline.fit(data).transform(data)\n",
    "\n",
    "processed_train_data.show(10)\n",
    "# prcesssed_train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "933e0bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+--------------------+-----------+---------------+-------------------+---------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+-----------------+--------------------+-----+-------------+--------------------+\n",
      "|author|     id|                text|        cleaned_text|text_length|author_category|    lexical_density|num_words|        cleaned_text|      lemmatized_txt|        cleaned_text|    lexical_density|    lexical_density|              tokens|     filtered_tokens|            word2vec|vectorized_tokens|               tfidf|label|   enc_author|            features|\n",
      "+------+-------+--------------------+--------------------+-----------+---------------+-------------------+---------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+-----------------+--------------------+-----+-------------+--------------------+\n",
      "|   MWS|id27763|How lovely is spr...|how lovely is spr...|        206|          wordy|0.00242801583636171|       34|how lovely is spr...|How lovely is spr...|how lovely is spr...|0.00242801583636171|0.00242801583636171|[how, lovely, is,...|[lovely, spring, ...|[0.04160118444512...|       (10,[],[])|          (10,[],[])|  1.0|(2,[1],[1.0])|[206.0,0.0,0.0,0....|\n",
      "|   MWS|id22965|A youth passed in...|a youth passed in...|        468|          wordy|0.00242801583636171|       83|a youth passed in...|A youth passed in...|a youth passed in...|0.00242801583636171|0.00242801583636171|[a, youth, passed...|[youth, passed, s...|[0.03956627875886...|       (10,[],[])|          (10,[],[])|  1.0|(2,[1],[1.0])|[468.0,0.0,0.0,0....|\n",
      "|   MWS|id00912|I confess that ne...|i confess that ne...|        144|          wordy|0.00242801583636171|       23|i confess that ne...|I confess that ne...|i confess that ne...|0.00242801583636171|0.00242801583636171|[i, confess, that...|[confess, neither...|[0.02857701263080...|       (10,[],[])|          (10,[],[])|  1.0|(2,[1],[1.0])|[144.0,0.0,0.0,0....|\n",
      "|   MWS|id16737|He shall find tha...|he shall find tha...|        106|          wordy|0.00242801583636171|       22|he shall find tha...|He shall find tha...|he shall find tha...|0.00242801583636171|0.00242801583636171|[he, shall, find,...|[shall, find, fee...|[0.04062317068455...|       (10,[],[])|          (10,[],[])|  1.0|(2,[1],[1.0])|[106.0,0.0,0.0,0....|\n",
      "|   MWS|id12799|He had escaped me...|he had escaped me...|        261|          wordy|0.00242801583636171|       47|he had escaped me...|He had escaped me...|he had escaped me...|0.00242801583636171|0.00242801583636171|[he, had, escaped...|[escaped, me,, mu...|[0.03074570410778...|   (10,[9],[1.0])|(10,[9],[3.583314...|  1.0|(2,[1],[1.0])|[261.0,0.0,0.0,0....|\n",
      "+------+-------+--------------------+--------------------+-----------+---------------+-------------------+---------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+-----------------+--------------------+-----+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/26 02:05:37 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 965:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# train, validation set split\n",
    "assembler = VectorAssembler(\n",
    "    # @todo re-add missing columns \n",
    "\n",
    "    # inputCols=[\n",
    "    #     \"text_length\", \"tfidf\",\"normalized_features\",'lexical_density',\n",
    "    #     'idx_author_category', 'num_words','density_x_words', 'word2vec'\n",
    "    # ],\n",
    "    inputCols=[\n",
    "        \"text_length\", \"tfidf\",\n",
    "        'num_words', 'word2vec'\n",
    "    ], \n",
    "    outputCol=\"features\")\n",
    "prepared_data = assembler.transform(processed_train_data)\n",
    "prepared_data = prepared_data.select(['features', 'label'])\n",
    "print(prepared_data.show(5))\n",
    "\n",
    "# train and validation split\n",
    "\n",
    "# author count and percentages\n",
    "# ratio\n",
    "ratio = {0: 0.25, 1: 0.25, 2: 0.25}\n",
    "\n",
    "# create the validation set with 25% of the entire data and keep distribution, stratified sampling\n",
    "df_test = prepared_data.stat.sampleBy('label', ratio, seed=40).cache()\n",
    "\n",
    "# subtract the validation set from the original training set to get 75% of the entire data \n",
    "df_train = prepared_data.subtract(df_test).cache()\n",
    "\n",
    "# check ratio\n",
    "total_count = df_train.count() + df_test.count()\n",
    "\n",
    "train_pctg = df_train.count() / total_count\n",
    "valid_pctg = df_test.count() / total_count  \n",
    "\n",
    "print(f'Ratio training:validation ={train_pctg}:{valid_pctg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033089dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML models\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "model_eval=RegressionEvaluator(metricName=\"rmse\",labelCol=\"label\")\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features',labelCol=\"idx_author\")\n",
    "\n",
    "lr_model = lr.fit(df_train)\n",
    "# predictions on validation set\n",
    "preds=lr_model.transform(df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
